input {
  file {
    path => "/var/log/td-agent/socker.*.log"
  }
}

filter {
  grok {
    match => {
      # Replace '@timestamp' with 'timestamp_fluentd' if you want to save when Fluentd wrote the log post
      # Note that '@timestamp' is different from 'timestamp',
      #   the first pattern is Logstash/Elasticsearch default field for timestamp, so we are basically just overwriting it
      "message" => [ "%{TIMESTAMP_ISO8601:@timestamp}%{SPACE}%{NOTSPACE:tag}%{SPACE}%{GREEDYDATA:json_data}" ]
    }
  }

  json {
    source => "json_data"

    # Parse the json-data into application.* fields
    # Remove this to parse the json in to the root-element, _doc.application.foo -> _doc.foo
    target => "application"

    # Remove the raw json-data
    remove_field => ["json_data" ]
  }


  mutate {
    # Copy/save when Logstash read the log post
    copy => { "@timestamp" => "timestamp_logstash"}

    # Keep the original message from Fluentd
    # The orginal message is the complete log-line from Fluentd, with timestamp + tag + log post
    # We don't need duplicates of this information, so we just rename the field
    rename => { "[application][log]" => "message"}

    # Move the application message into the message field
    #rename => { "[application][message]" => "message"}
  }


  date {
    # Use the applications timestamp as main timestamp
    match => ["[application][@timestamp]", "ISO8601"]
  }
}

output {
  elasticsearch {
    hosts => ["{{ hostvars['logserver.snorbu.se']['ansible_host'] }}:9200"]
    index => "rocker-%{+YYYY.MM.dd}"
  }
  stdout { codec => line }
}


